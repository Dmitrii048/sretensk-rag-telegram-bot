import os
import requests
from bs4 import BeautifulSoup 
from urllib.parse import urljoin, urlparse
# –ò–º–ø–æ—Ä—Ç—ã LangChain
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
# ================= –ù–ê–°–¢–†–û–ô–ö–ò =================
SOURCE_FOLDER = "data1" # –¢–≤–æ—è –ø–∞–ø–∫–∞ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
DB_PATH = "sretensk_db" # –ü–∞–ø–∫–∞, –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è –±–∞–∑–∞ (—Ç–∞ –∂–µ, —á—Ç–æ –≤ –±–æ—Ç–µ!)
# –°—Å—ã–ª–∫–∏, —Å –∫–æ—Ç–æ—Ä—ã—Ö –Ω–∞—á–Ω–µ–º –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞ + –≤—Å–µ –≤–∞–∂–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã
START_URLS = [
    # –ì–ª–∞–≤–Ω–∞—è –∏ –æ–±—â–∏–µ
    "https://sdamp.ru/",
    "https://sdamp.ru/contacts/",

    # –û–± –ê–∫–∞–¥–µ–º–∏–∏
    "https://sdamp.ru/about/",
    "https://sdamp.ru/about/akademia-segodnya/",
    "https://sdamp.ru/about/sotrudnichestvo/",
    "https://sdamp.ru/about/shkola-abiturienta/",

    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ê–∫–∞–¥–µ–º–∏–∏
    "https://sdamp.ru/structure/",
    "https://sdamp.ru/structure/scheme/",
    "https://sdamp.ru/structure/rector/",
    "https://sdamp.ru/structure/administratsiya-sotrudniki/",
    "https://sdamp.ru/structure/uchenyy-sovet/",
    "https://sdamp.ru/structure/cathedra/",
    "https://sdamp.ru/structure/cathedra/theology/",
    "https://sdamp.ru/structure/cathedra/pastyr/",
    "https://sdamp.ru/structure/cathedra/history/",
    "https://sdamp.ru/structure/cathedra/yazyk/",
    "https://sdamp.ru/structure/cathedra/prakt-gum/",
    "https://sdamp.ru/structure/other-structures/",
    "https://sdamp.ru/life/studencheskiy-sovet/",

    # –°–≤–µ–¥–µ–Ω–∏—è –æ–± –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
    "https://sdamp.ru/sveden/",
    "https://sdamp.ru/sveden/common/",
    "https://sdamp.ru/sveden/struct/",
    "https://sdamp.ru/sveden/document/",
    "https://sdamp.ru/sveden/education/",
    "https://sdamp.ru/sveden/managers/",
    "https://sdamp.ru/sveden/employees/",
    "https://sdamp.ru/sveden/objects/",
    "https://sdamp.ru/sveden/paid_edu/",
    "https://sdamp.ru/sveden/budget/",
    "https://sdamp.ru/sveden/vacant/",
    "https://sdamp.ru/sveden/grants/",
    "https://sdamp.ru/sveden/inter/",
    "https://sdamp.ru/sveden/catering/",
    "https://sdamp.ru/sveden/eduStandarts/",
    "https://sdamp.ru/sveden/quality/",
    "https://sdamp.ru/sveden/eios/",

    # –ê–±–∏—Ç—É—Ä–∏–µ–Ω—Ç—É –∏ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
    "https://sdamp.ru/abitur/",
    "https://sdamp.ru/abitur/bachelor/",
    "https://sdamp.ru/abitur/magistr/",
    "https://sdamp.ru/abitur/aspirant/",
    "https://sdamp.ru/abitur/faq/",
    "https://sdamp.ru/obrazovanie/bakalavriat/",
    "https://sdamp.ru/obrazovanie/magistratura/",
    "https://sdamp.ru/obrazovanie/aspirantura/",
    "https://sdamp.ru/obrazovanie/prepodavateli/",
    "https://sdamp.ru/obrazovanie/raspisanie-zanyatiy/–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ%20–ë–∞–∫%20–∏%20–ú–∞–≥%202026.pdf",

    # –ù–∞—É–∫–∞
    "https://sdamp.ru/nauka/biblioteka/",
    "https://sdamp.ru/nauka/conferences/",
    "https://sdamp.ru/nauka/sretenskiy-sbornik/",
    "https://sdamp.ru/nauka/sretenskoe-slovo/",
    "https://sdamp.ru/nauka/zhurnal-diakrisis/",
    "https://sdamp.ru/nauka/dokt-sovet/",
    "https://sdamp.ru/nauka/stud-nauka/",

    # –í—Å–µ –≤–∞–∂–Ω—ã–µ PDF-–¥–æ–∫—É–º–µ–Ω—Ç—ã (–¥–æ–±–∞–≤–ª–µ–Ω—ã –Ω–∞–ø—Ä—è–º—É—é, —Ç.–∫. –∫—Ä–∞—É–ª–µ—Ä –∏—Ö –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç)
    "https://sdamp.ru/upload/–ü–µ—Ä–µ—á–µ–Ω—å_–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤_–Ω–∞_–∫–æ–Ω–∫—É—Ä—Å_–Ω–∞_–∑–∞–º–µ—â–µ–Ω–∏–µ_–¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π_–ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏—Ö_2025.pdf",
    "https://sdamp.ru/upload/–ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ_—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è_–∫_–¥–æ–ª–∂–Ω–æ—Å—Ç—è–º_–ø–µ–¥–∞–≥–æ–≥–∏—á–µ—Å–∫–∏—Ö_—Ä–∞–±–æ—Ç–Ω–∏–∫–æ–≤_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/ris/Informaciya_o_predostavlenii_platnyx_obrazovatelynyx_uslug_tolyko_za_schet_sredstv_Akademii.pdf",
    "https://sdamp.ru/vikon/sveden/files/aij/Dogovor_ob_obrazovanii_511_Teologiya_byudghet_graghdane_RF_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/viy/Dogovor_ob_obrazovanii_511_Teologiya_byudghet_inostrannye_graghdane_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/aza/Dogovor_ob_obrazovanii_511_Teologiya_platnoe_obuchenie_inostrannye_graghdane_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/zik/Dogovor_ob_obrazovanii_000_Podgotovka_(aspirantura)_graghdane_RF_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/vig/Dogovor_ob_obrazovanii_000_Podgotovka_(aspirantura)_inostrannye_graghdane_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/viy/Dogovor_ob_obrazovanii_480301_Teologiya_byudghet_graghdane_RF_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/eiu/Dogovor_ob_obrazovanii_480301_Teologiya_byudghet_inostrannye_graghdane_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/vig/Dogovor_ob_obrazovanii_480301_Teologiya_platnoe_obuchenie_graghdane_RF_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/aiq/Dogovor_ob_obrazovanii_480301_Teologiya_platnoe_obuchenie_inostrannye_graghdane_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/vip/Dogovor_ob_obrazovanii_000000_Podgotovka_(bakalavriat)_graghdane_RF_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/vic/Dogovor_ob_obrazovanii_000000_Podgotovka_(bakalavriat)_inostrannye_graghdane_2025.pdf",
    "https://sdamp.ru/vikon/sveden/files/aip/Prikaz_ob_ustanovlenii_stoimosti_POU_na_2025-2026_uchebnyi_god_(1_kurs).pdf",
    "https://sdamp.ru/vikon/sveden/files/rih/Prikaz_ot_30.12.2025_No_367_Ob_ustanovlenii_razmerov_GAS,_PGAS,_GSS_i_PGSS_studentam,_GS_aspirantam_i_materialynoi_podderghki_obuchayuschimsya.pdf",
    "https://sdamp.ru/vikon/sveden/files/eiw/Prikaz_ot_30.08.2024_No_166_Ob_utverghdenii_sostava_socialyno-stipendialynoi_komissii_Akademii.pdf",
    "https://sdamp.ru/vikon/sveden/files/ais/Pologhenie_o_stipendialynom_obespechenii_studentov_(ot_29-08-2024_GHurnal_(protokol)_No_8_(51).pdf",
    "https://sdamp.ru/sveden/quality/federalnyj-zakon-ot-29-dekabrya-2012-g-n-273-fz-ob-obrazovanii-v-rf.pdf",
    "https://sdamp.ru/sveden/quality/0001202009070046.pdf",
    "https://sdamp.ru/sveden/quality/–ú–µ—Ç–æ–¥–†–µ–∫–æ–º_–ø–æ_—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏_–û–û_–º–µ—Ö–∞–Ω–∏–∑–º–æ–≤_–∫–∞—á–µ—Å—Ç–≤–∞.pdf",
    "http://government.ru/docs/all/109497/",
]
MAX_DEPTH = 2 # –ì–ª—É–±–∏–Ω–∞ 2: –ì–ª–∞–≤–Ω–∞—è -> –°—Å—ã–ª–∫–∞ –Ω–∞ –Ω–µ–π -> –°—Å—ã–ª–∫–∞ –Ω–∞ –Ω–µ–π
MAX_PAGES = 50 # –õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü, —á—Ç–æ–±—ã –Ω–µ –∫–∞—á–∞—Ç—å –≤–µ—Å—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
# ================= –õ–û–ì–ò–ö–ê =================
def get_links_from_page(url, domain="sdamp.ru"):
    """–ò—â–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –≤–µ–¥—É—â–∏–µ –≤–Ω—É—Ç—Ä—å –∞–∫–∞–¥–µ–º–∏–∏"""
    links = set()
    try:
        # verify=False –Ω—É–∂–µ–Ω, –µ—Å–ª–∏ —É —Å–∞–π—Ç–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏
        response = requests.get(url, timeout=10, verify=False)
        soup = BeautifulSoup(response.content, "html.parser")
       
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            full_url = urljoin(url, href)
           
            # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä –∏ –≤–Ω–µ—à–Ω–∏–µ —Å—Å—ã–ª–∫–∏
            if domain in full_url and not full_url.endswith(('.pdf', '.docx', '.jpg', '.png', '#')):
                links.add(full_url)
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å—Å—ã–ª–∫–∏ —Å {url}: {e}")
    return links
def crawl_site(start_urls, depth=1):
    """–ü–∞—É–∫ –¥–ª—è —Å–∞–π—Ç–∞"""
    visited = set()
    queue = [(url, 1) for url in start_urls]
    final_list = []
   
    print(f"üï∑ –ó–∞–ø—É—Å–∫–∞—é –æ–±—Ö–æ–¥ —Å–∞–π—Ç–∞ (–ì–ª—É–±–∏–Ω–∞: {depth})...")
   
    while queue and len(visited) < MAX_PAGES:
        url, current_depth = queue.pop(0)
       
        if url in visited: continue
        visited.add(url)
        final_list.append(url)
        print(f" üîó –î–æ–±–∞–≤–ª–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
       
        if current_depth < depth:
            new_links = get_links_from_page(url)
            for link in new_links:
                if link not in visited:
                    queue.append((link, current_depth + 1))
                   
    return final_list
def create_knowledge_base():
    documents = []
   
    # 1. –û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–û–í (—Å OCR)
    print(f"\nüìÇ 1. –°–∫–∞–Ω–∏—Ä—É—é –ø–∞–ø–∫—É '{SOURCE_FOLDER}'...")
    if os.path.exists(SOURCE_FOLDER):
        for filename in os.listdir(SOURCE_FOLDER):
            file_path = os.path.join(SOURCE_FOLDER, filename)
            if filename.startswith("~$"): continue # –ü—Ä–æ–ø—É—Å–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            try:
                if filename.lower().endswith(".pdf"):
                    print(f" üìÑ PDF (OCR –≤–∫–ª—é—á–µ–Ω): {filename}")
                    # extract_images=True –≤–∫–ª—é—á–∞–µ—Ç OCR (—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å–∫–∞–Ω–æ–≤)
                    loader = PyPDFLoader(file_path, extract_images=True)
                    documents.extend(loader.load())
                   
                elif filename.lower().endswith(".docx"):
                    print(f" üìù Word: {filename}")
                    loader = Docx2txtLoader(file_path)
                    documents.extend(loader.load())
                   
                elif filename.lower().endswith(".txt"):
                    print(f" üìú Txt: {filename}")
                    loader = TextLoader(file_path, encoding="utf-8")
                    documents.extend(loader.load())
            except Exception as e:
                print(f" ‚ùå –û—à–∏–±–∫–∞ —Ñ–∞–π–ª–∞ {filename}: {e}")
    else:
        print("‚ö†Ô∏è –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
    # 2. –û–ë–†–ê–ë–û–¢–ö–ê –°–ê–ô–¢–ê
    print(f"\nüåê 2. –°–∫–∞–Ω–∏—Ä—É—é —Å–∞–π—Ç...")
    target_urls = crawl_site(START_URLS, MAX_DEPTH)
    print(f" ‚úÖ –í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —á—Ç–µ–Ω–∏—è: {len(target_urls)}")
   
    if target_urls:
        try:
            loader = WebBaseLoader(target_urls)
            loader.requests_kwargs = {'verify': False}
            web_docs = loader.load()
            documents.extend(web_docs)
            print(f" üì• –¢–µ–∫—Å—Ç —Å —Å–∞–π—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ.")
        except Exception as e:
            print(f" ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–∞–π—Ç–∞: {e}")
    # 3. –°–û–ó–î–ê–ù–ò–ï –í–ï–ö–¢–û–†–û–í
    if not documents:
        print("‚ùå –ù–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç.")
        return
    print(f"\nüß† 3. –°–æ–∑–¥–∞—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π (–≤—Å–µ–≥–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)...")
   
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documents)
   
    # –í–ê–ñ–ù–û: –ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–∞ –∂–µ, —á—Ç–æ –∏ –≤ –±–æ—Ç–µ!
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
   
    db = FAISS.from_documents(chunks, embeddings)
    db.save_local(DB_PATH)
   
    print(f"\nüéâ –ì–û–¢–û–í–û! –ë–∞–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫—É '{DB_PATH}'")
    print("–¢–µ–ø–µ—Ä—å –Ω—É–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞ GitHub.")
if __name__ == "__main__":
    create_knowledge_base()
